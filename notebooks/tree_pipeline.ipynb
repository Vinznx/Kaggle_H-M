{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee6fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33a5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = \"../data/h-and-m-personalized-fashion-recommendations/\"\n",
    "N_POPULAR_CANDIDATES = 12  # Number of popular items to recommend\n",
    "NEG_PER_USER = 5 #negative sample per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233f5041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Data Loading ---\n",
    "\n",
    "print(\"1. Loading Data...\")\n",
    "\n",
    "# Load core files\n",
    "df_trans = pd.read_csv(\n",
    "    DATA_PATH + \"transactions_train.csv\",\n",
    "    usecols=[\"customer_id\", \"article_id\", \"t_dat\"]\n",
    ")\n",
    "df_articles = pd.read_csv(\n",
    "    DATA_PATH + \"articles.csv\", \n",
    "    usecols=['article_id', 'product_type_no', 'product_group_name', 'department_no']\n",
    ")\n",
    "df_customers = pd.read_csv(\n",
    "    DATA_PATH + \"customers.csv\", \n",
    "    usecols=['customer_id', 'age', 'club_member_status']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521bd11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Feature Engineer...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Feature Engineer ---\n",
    "\n",
    "print(\"2. Feature Engineer...\")\n",
    "\n",
    "#encode id (for tree model)\n",
    "all_users = df_customers[\"customer_id\"].unique()\n",
    "all_items = df_articles[\"article_id\"].unique()\n",
    "\n",
    "# user → index\n",
    "user_to_idx = {u: i for i, u in enumerate(all_users)}\n",
    "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
    "\n",
    "# item → index\n",
    "item_to_idx = {a: i for i, a in enumerate(all_items)}\n",
    "idx_to_item = {i: a for a, i in item_to_idx.items()}\n",
    "\n",
    "all_user_id = pd.Series(all_users).map(user_to_idx)\n",
    "all_item_id = pd.Series(all_items).map(item_to_idx)\n",
    "\n",
    "df_customers[\"customer_id\"] = df_customers[\"customer_id\"].map(user_to_idx)\n",
    "df_articles[\"article_id\"] = df_articles[\"article_id\"].map(item_to_idx)\n",
    "df_trans[\"article_id\"] = df_trans[\"article_id\"].map(item_to_idx)\n",
    "df_trans[\"customer_id\"] = df_trans[\"customer_id\"].map(user_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb265bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Data prepare...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Data prepare ---\n",
    "\n",
    "print(\"3. Data prepare...\")\n",
    "\n",
    "#create dataset with positive sample\n",
    "df_pos_data = pd.DataFrame()\n",
    "df_pos_data[\"customer_id\"] = df_trans[\"customer_id\"]\n",
    "df_pos_data[\"article_id\"] = df_trans[\"article_id\"]\n",
    "df_pos_data[\"label\"] = 1\n",
    "\n",
    "#create dataset with negative sample\n",
    "\n",
    "rep_users = np.repeat(all_user_id, NEG_PER_USER)\n",
    "sampled_items = np.random.choice(all_item_id, size=len(rep_users), replace=True)\n",
    "\n",
    "df_neg_data= pd.DataFrame()\n",
    "df_neg_data[\"customer_id\"] = rep_users\n",
    "df_neg_data[\"article_id\"] = sampled_items\n",
    "df_neg_data[\"label\"] = 0\n",
    "\n",
    "#merge pos and neg dataset\n",
    "df_all = pd.concat([df_pos_data, df_neg_data], ignore_index=True)\n",
    "\n",
    "X = df_all[[\"customer_id\", \"article_id\"]]\n",
    "y = df_all[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,          \n",
    "    random_state=42        # for reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa170861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Apply the tree model...\n",
      "Sample predictions: [0.85033614 0.8929093  0.69820417 0.82246548 0.8165944 ]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Apply the tree model ---\n",
    "\n",
    "print(\"4. Apply the tree model...\")\n",
    "\n",
    "train_data = lgb.Dataset(X_train, y_train)\n",
    "valid_data = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",        # root mean squared error\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=500\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Sample predictions:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5cc67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. make the submission...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6860 [03:26<131:27:52, 69.02s/it]"
     ]
    }
   ],
   "source": [
    "# --- 5. make the submission ---\n",
    "\n",
    "print(\"5. make the submission...\")\n",
    "\n",
    "def recommend_for_users_batch(user_idx_batch, model, num_items, topk=12):\n",
    "    batch_size = len(user_idx_batch)\n",
    "\n",
    "    user_column = np.repeat(user_idx_batch, num_items)      #Bxnum_items\n",
    "    item_column = np.tile(np.arange(num_items), batch_size) #Bxnum_items\n",
    "\n",
    "    df_tmp = pd.DataFrame({\n",
    "        \"user_idx\": user_column,\n",
    "        \"item_idx\": item_column\n",
    "    })\n",
    "\n",
    "    #predict all at once\n",
    "    scores = model.predict(df_tmp)\n",
    "    scores = scores.reshape(batch_size, num_items)\n",
    "\n",
    "    #get top k items\n",
    "    topk_idx = np.argpartition(-scores, topk, axis=1)[:, :topk]\n",
    "\n",
    "    return topk_idx\n",
    "\n",
    "batch_size = 200\n",
    "num_all_tiems = len(all_items)\n",
    "# Precompute item lookup array (string type)\n",
    "item_lookup = np.array([idx_to_item[i] for i in range(num_all_items)], dtype=str)\n",
    "submission_rows = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_user_id), batch_size)):\n",
    "    batch_users = all_user_id[i:i+batch_size]\n",
    "    batch_size_actual = len(batch_users)\n",
    "\n",
    "    #predict\n",
    "    batch_top_items = recommend_for_users_batch(batch_users, model, num_all_tiems, topk=12)\n",
    "    batch_top_items = np.array(batch_top_items) # B x topk\n",
    "\n",
    "    decoded_items = item_lookup[batch_top_items] # B x topk\n",
    "\n",
    "    pred_strs = [\" \".join(row) for row in decoded_items]\n",
    "\n",
    "    user_ids = [idx_to_user[u] for u in batch_users]\n",
    "\n",
    "    submission_rows.extend(zip(user_ids, pred_strs))\n",
    "\n",
    "submission = pd.DataFrame(submission_rows, columns=[\"customer_id\", \"prediction\"])\n",
    "submission.to_csv(\"lgb_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e995d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[np.int64(108775015), np.int64(399223034), np.int64(399223004), np.int64(399223015), np.int64(399136011), np.int64(399223025), np.int64(399223026), np.int64(399223028), np.int64(399223029), np.int64(399223030), np.int64(399223032), np.int64(399201045)]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(pred_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35a4cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108775015, 108775044, 108775051, ..., 956217002, 957375001,\n",
       "       959461001])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2231cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
